{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Wolfe_learning_rate(f, gradf, x, d, s0=5, eps1 = 1e-4, eps2 = 0.99, itermax = 20):\n",
    "#     d : search direction\n",
    "#     s0 : first approximation of learning rate\n",
    "#     0 < esp1 < eps2 < 1\n",
    "#     k := 0 ; s− = 0 ; s+ = +∞;\n",
    "\n",
    "    iter, sn , sp = 0,0, 1e8\n",
    "#     sn, sp are the minorant and majorant of learning rate s\n",
    "    grad_x = gradf(x)\n",
    "    f_x = f(x)\n",
    "    s = s0 \n",
    "    cond1 = f(x+s*d) <= f_x + eps1*s*grad_x.T.dot(d)\n",
    "    cond2 = gradf(x+s*d).T.dot(d) >= eps2*grad_x.T.dot(d)\n",
    "    while not cond1 or not cond2 :\n",
    "        if not cond1: \n",
    "            sp = s\n",
    "            s = (sn + sp)/2.\n",
    "        elif not cond2:\n",
    "            sn = s\n",
    "            if sp < 1e8:\n",
    "                s = (sn + sp)/2.\n",
    "            else:\n",
    "                s = 2*s \n",
    "        cond1 = f(x+s*d) <= f_x + eps1*s*grad_x.T.dot(d)\n",
    "        cond2 = gradf(x+s*d).T.dot(d) >= eps2*grad_x.T.dot(d)\n",
    "        iter += 1 \n",
    "        if iter > itermax:\n",
    "            break\n",
    "    return s , iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy line_search :  1.0\n",
      "Homemade line_search:  0.625\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import line_search\n",
    "import numpy as np\n",
    "def obj_func(x):\n",
    "    return (x[0])**2+(x[1])**2\n",
    "def obj_grad(x):\n",
    "    return np.array([2*x[0], 2*x[1]])\n",
    "start_point = np.array([1.8, 1.7])\n",
    "search_gradient = np.array([-1.0, -1.0])\n",
    "print(\"Scipy line_search : \", line_search(obj_func, obj_grad, start_point,search_gradient )[0])\n",
    "print(\"Homemade line_search: \",Wolfe_learning_rate(obj_func,obj_grad,start_point, -obj_grad(start_point))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy line_search :  0.12564991334488734\n",
      "Homemade line_search:  0.15625\n"
     ]
    }
   ],
   "source": [
    "def objective(x):\n",
    "    return x[0]**2 + 4*x[1]**2\n",
    "def grad(x):\n",
    "    return 2*x*np.array([1, 4])\n",
    "x = np.array([1., 3.]) #current point\n",
    "p = -grad(x) #current search direction\n",
    "a = line_search(objective, grad, x, p)[0]\n",
    "print(\"Scipy line_search : \",a)\n",
    "print(\"Homemade line_search: \",Wolfe_learning_rate(objective,grad,x, p)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as npl \n",
    "def BFGS(f,gradf,x0, B0):\n",
    "#     H0 is random symetric definite positive matrix\n",
    "    B = B0\n",
    "    iter = 0\n",
    "    d = -B.dot(gradf(x))\n",
    "    s = Wolfe_learning_rate(f, gradf, x, d)\n",
    "    x = x - s*B.dot(gradf(x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'myscript.py'\n",
    "%run -i 'poisson_disc.py'\n",
    "N = 401\n",
    "pi = importIMG('../images/circle.png', N, show = False)\n",
    "\n",
    "xgrid = (np.arange(N)+0.5)/N - 0.5\n",
    "ygrid = (np.arange(N)+0.5)/N - 0.5\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
